################################################################################

# Outlier Removal:

# 	crop off the top 10% using residual error using enron_outliers.py

# 	Just for experimentational purposes, I decided to use a standard deviation 
# 	based logic system in order to discriminate against outliers, instead of 
# 	simply cutting any x% off the top of any feature-column's values. This 
# 	seeme too subjective to me

# 	the system lists the value, and position of any column item if it exceeds a 
# 	threshold in terms of that column's respective standard deviation. it also
# 	logs that columns's respective mean and standard deviation.

# 	once this is accomplished, it is then turned into a dictionarys' key's 
# 	value (the key being the respective feature name).

# 	the format is as such:

# 	{feature_1: [(position,
#                  	value,
#                  	mean,
#                  	std),
#                  	...
#                  ],

#      feature_2: [(position,
#                  	value,
#                  	mean,
#                  	std),
#                  	...
#                  ]
     
#       ...
#       }

# to remove all rows of our array that contain outliers we should first notice 
# how some of these lines contain more than one feature with outlier values


################################################################################

# try out different classifiers with a voting system along to experiment with 
# the potential accuracy of said classifiers whe working together.

from collections import Counter
import sys
import pickle
import pandas as pd
import pprint as pp
from sklearn import svm, cross_validation, neighbors
from sklearn.ensemble import VotingClassifier, RandomForestClassifier

outliers = pickle.load(open("Outliers_dictionary.pkl","r"))

sys.path.append("../tools/")

from feature_format import featureFormat, targetFeatureSplit
from tester import dump_classifier_and_data

features_list = ['poi',
				 'salary',
				 'bonus', 
				 'deferral_payments',
				 'director_fees',
				 'exercised_stock_options',
				 'expenses',
				 'from_messages',
				 'from_poi_to_this_person',
				 'from_this_person_to_poi',
				 'loan_advances',
				 'long_term_incentive',
				 'restricted_stock',
				 'restricted_stock_deferred',
				 'to_messages',
				 'total_payments',
				 'total_stock_value'] 

with open("final_project_dataset.pkl", "r") as data_file:
    data_dict = pickle.load(data_file)

data_dict = pickle.load(open("../final_project/final_project_dataset.pkl", "r"))
features = features_list
data = featureFormat(data_dict, features)
df = pd.DataFrame(data, columns = features_list)

def do_ml(df):
    X, y= df.drop('poi', axis = 1).values, df.poi.values 

    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X,
                                                        y,
                                                        test_size=0.25)

    clf = VotingClassifier([('lsvc',svm.LinearSVC()),
                            ('knn',neighbors.KNeighborsClassifier()),
                            ('rfor',RandomForestClassifier())])


    clf.fit(X_train, y_train)
    confidence = clf.score(X_test, y_test)
    predictions = clf.predict(X_test)
    print "################################################################"\
    "################################################################"
    print "Here we demonstrate the potential accuracy of voting classifier"\
    " (composed of many other classifiers) before we create new features."
    print
    print 'accuracy: '
    print confidence
    print
    print'predicted class counts: '
    print Counter(predictions)
    print
    print "################################################################"\
    "################################################################"

do_ml(df)

################################################################################

# Now let's tryout making new Features:
# Let's begin by using tfidf


#     with pca in order to discover which are the best features with each 
#     classifier.